#+OPTIONS: ':nil -:nil ^:{} num:t toc:nil
#+AUTHOR: Hiroyuki Yamada
#+CREATOR: Emacs 26.1 (Org mode 9.1.14 + ox-hugo)
#+HUGO_WITH_LOCALE:
#+HUGO_FRONT_MATTER_FORMAT: toml
#+HUGO_LEVEL_OFFSET: 1
#+HUGO_PRESERVE_FILLING:
#+HUGO_DELETE_TRAILING_WS:
#+HUGO_SECTION: .
#+HUGO_BUNDLE:
#+HUGO_BASE_DIR: ./
#+HUGO_CODE_FENCE:
#+HUGO_USE_CODE_FOR_KBD:
#+HUGO_PREFER_HYPHEN_IN_TAGS:
#+HUGO_ALLOW_SPACES_IN_TAGS:
#+HUGO_AUTO_SET_LASTMOD:
#+HUGO_CUSTOM_FRONT_MATTER:
#+HUGO_BLACKFRIDAY:
#+HUGO_FRONT_MATTER_KEY_REPLACE:
#+HUGO_DATE_FORMAT: %Y-%m-%dT%T+09:00
#+HUGO_PAIRED_SHORTCODES:
#+HUGO_PANDOC_CITATIONS:
#+BIBLIOGRAPHY:
#+HUGO_ALIASES:
#+HUGO_AUDIO:
#+DATE: <2019-02-10 Sun>
#+DESCRIPTION:
#+HUGO_DRAFT:
#+HUGO_EXPIRYDATE:
#+HUGO_HEADLESS:
#+HUGO_IMAGES:
#+HUGO_ISCJKLANGUAGE:
#+KEYWORDS:
#+HUGO_LAYOUT:
#+HUGO_LASTMOD:
#+HUGO_LINKTITLE:
#+HUGO_LOCALE:
#+HUGO_MARKUP:
#+HUGO_MENU:
#+HUGO_MENU_OVERRIDE:
#+HUGO_OUTPUTS:
#+HUGO_PUBLISHDATE:
#+HUGO_SERIES:
#+HUGO_SLUG:
#+HUGO_TAGS:
#+HUGO_CATEGORIES:
#+HUGO_RESOURCES:
#+HUGO_TYPE:
#+HUGO_URL:
#+HUGO_VIDEOS:
#+HUGO_WEIGHT: auto

#+STARTUP: showall logdone

* DONE cpprb
CLOSED: [2021-01-21 Thu 21:22]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:UNNUMBERED: t
:END:

[[https://ymd_h.gitlab.io/cpprb/][https://ymd_h.gitlab.io/cpprb/images/logo.png]]

#+html: {{% button-center href="/cpprb/installation/" icon="fas fa-download" icon-position="right" %}}Get Started{{% /button-center %}}

---

** Make Reinforcement Learning Easier
:PROPERTIES:
:UNNUMBERED: t
:END:

[[/cpprb/images/replay_buffer.png]]

Experience Replay is widely used for off-policy reinforcement
learning. With cpprb, you can start your experiment quickly without
implementing troublesome replay buffer.

---

** Fast & Flexible
:PROPERTIES:
:UNNUMBERED: t
:END:

{{< container >}}{{< row >}}{{< col size="md">}}

{{% markdownify %}}
*** Native
Heavy calculation is implemented with C++ and [[https://cython.org/][Cython]]. cpprb is usually
faster than Python naive implementation.
{{% /markdownify %}}

#+html: {{% button-center href="/cpprb/comparison/benchmark/" icon="fas fa-angle-double-right" icon-position="right" %}}Learn More{{% /button-center %}}

{{< /col >}}{{< col size="md">}}

{{% markdownify %}}
*** Multiprocessing
cpprb supports [[https://arxiv.org/abs/1803.00933][Ape-X]] on single computer. You don't need to think
problematic lock. cpprb locks only critical section internally well.
{{% /markdownify %}}

#+html: {{% button-center href="/cpprb/features/ape-x/" icon="fas fa-angle-double-right" icon-position="right" %}}Learn More{{% /button-center %}}

{{< /col >}}{{< col size="md">}}

{{% markdownify %}}
*** Flexible Environment
cpprb adopts flexible environment. Any numbers of [[https://numpy.org/][Numpy]] compatible
environment values can be stored.
{{% /markdownify %}}

#+html: {{% button-center href="/cpprb/features/flexible_environment/" icon="fas fa-angle-double-right" icon-position="right" %}}Learn More{{% /button-center %}}

{{< /col >}}{{< col size="md">}}

{{% markdownify %}}
*** Framework Free
You can build your own reinforcement learning algorithms together with
your favorite deep learning library (e.g. [[https://www.tensorflow.org/][TensorFlow]], [[https://pytorch.org/][PyTorch]]).
{{% /markdownify %}}


{{< /col >}} {{< /row >}} {{< /container >}}

---

** Ecosystem & Community
:PROPERTIES:
:UNNUMBERED: t
:END:

{{< container >}} {{< row >}} {{< col size="md">}}

{{% markdownify %}}
*** Duscussion
Any questions, requests, and so on are welcome.
{{% /markdownify %}}

#+html: {{% button-center href="https://github.com/ymd-h/cpprb/discussions" icon="fas fa-comments" icon-position="right" %}}User Forum{{% /button-center %}}

{{< /col >}} {{< col size="md">}}

{{% markdownify %}}
*** TF2RL
TF2RL provides a set of reinforcement learning algorithms for TensorFlow 2.
TF2RL uses cpprb for off-policy algorithm.
{{% /markdownify %}}

#+html: {{% button-center href="https://github.com/keiohta/tf2rl" icon="fab fa-github" icon-position="right" %}}TF2RL{{% /button-center %}}

{{< /col >}} {{< /row >}} {{< /container >}}

* Installation
:PROPERTIES:
:EXPORT_HUGO_SECTION*: installation
:END:

** DONE Step by step installation on macOS
CLOSED: [2020-01-17 Fri 21:09]
:PROPERTIES:
:EXPORT_FILE_NAME: install_on_macos
:END:

Since clang (libc++) has not implemented array specialization of
=std::shared_ptr= ([[http://libcxx.llvm.org/cxx1z_status.html][libc++ C++17 Status]], P0414R2 etc.), cpprb cannot be
compiled by clang.

Here we describe how to install cpprb on macOS using [[https://www.macports.org/][MacPorts]].

#+begin_src shell
sudo port selfupdate
sudo port install gcc9
sudo port select gcc mp-gcc9
git clone https://gitlab.com/ymd_h/cpprb.git
cd cpprb
CC=/opt/local/bin/g++ CXX=/opt/local/bin/g++ pip install .
#+end_src

* Features
:PROPERTIES:
:EXPORT_HUGO_SECTION*: features
:END:

** DONE Flexible Environment
CLOSED: [2019-11-08 Fri 05:58]
:PROPERTIES:
:EXPORT_FILE_NAME: flexible_environment
:END:

*** Overview

In ~cpprb~ version 8 and newer, you can store any number of
environments (aka. observation, action, etc.).

For example, you can add your special environments like
~next_next_obs~, ~second_reward~, and so on.

These environments can take multi-dimensional shape (e.g. ~3~,
~(4,4)~, ~(84,84,4)~), and any [[https://numpy.org/devdocs/user/basics.types.html][numpy data type]].


**** ~__init__~
In order to construct replay buffers, you need to specify the second
parameter of their constructor, ~env_dict~.

The ~env_dict~ is a ~dict~ whose keys are environment name and whose
values are ~dict~ describing their properties.

The following table is supported properties and their default values.

| key   | description                    | type                         | default value                                    |
|-------+--------------------------------+------------------------------+--------------------------------------------------|
| shape | shape (size of each dimension) | ~int~ or array like of ~int~ | ~1~                                              |
| dtype | data type                      | ~numpy.dtype~                | ~default_dtype~ in constructor or ~numpy.single~ |

**** ~add~
When ~add~ -ing environments to the replay buffer, you have to pass
them by keyword arguments (aka. ~key=value~ style). If your
environment name is not a syntactically valid identifier, you can
still create dictionary first, then unpack the dictionary by ~**~
operator (e.g. ~rb.add(**kwargs)~).

**** ~sample~
~sample~ returns ~dict~ with keys of environments' name and with
values of sampled ones.


*** Example Usage

#+begin_src python
from cpprb import ReplayBuffer
import numpy as np

buffer_size = 32

rb = ReplayBuffer(buffer_size,{"obs": {"shape": (4,4)},
                               "act": {"shape": 1},
                               "rew": {},
                               "next_obs": {"shape": (4,4)},
                               "next_next_obs": {"shape": (4,4)},
                               "done": {},
                               "my_important_info": {"dtype": {np.short}}})

for _ in range(100):
    rb.add(obs=np.zeros((4,4)),
           act=1.5,
           rew=0.0,
           next_obs=np.zeros((4,4)),
           next_next_obs=np.zeros((4,4)),
           done=0,
           my_important_info=2)

rb.sample(64)
#+end_src
*** Notes
~priorities~, ~weights~, and ~indexes~ for ~PrioritizedReplayBuffer~
are special environments and are automatically set.


*** Technical Detail
Internally, these flexible environments are implemented with (cython
version of) ~numpy.ndarray~. They were implemented with C++ code in
older than version 8, which had trouble in flexibilities of data type
and the number of environment. (There was a dirty hack to put all
extra environments into ~act~ which was not treat specially.)


** DONE Multistep-add
CLOSED: [2019-11-10 Sun 14:08]
:PROPERTIES:
:EXPORT_FILE_NAME: multistep_add
:END:

*** Overview
In cpprb, you can add multistep environment to replay buffer simultaneously.


*** Example Usage
#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

rb = ReplayBuffer(32,{"obs": {"shape": 5},
                      "act": {"shape": 3},
                      "rew": {},
                      "next_obs": {"shape": 5},
                      "done": {}})

steps = 10

rb.get_stored_size() # -> 0


rb.add(obs=np.ones(steps,5),
       act=np.zeros(steps,3),
       rew=np.ones(steps),
       next_obs=np.ones(steps,5),
       done=np.zeros(steps))


rb.get_stored_size() # -> steps
#+end_src
*** Notes
The dimension for step must be 0th dimension

*** Technical Detail
The shapes for ~add~ for every environments are stored as
~add_shape=(-1,*env_shape)~ at constructor, s.t. ~env_shape~ is the environment
shape.

Only one environment value is used to determine the step size by
reshaping to ~add_shape~, so that user must pass the values of the
same step size.


** DONE Prioritized Experience Replay
CLOSED: [2019-11-10 Sun 13:26]
:PROPERTIES:
:EXPORT_FILE_NAME: PER
:END:

*** Overview
Prioritized experience replay was proposed by [[https://arxiv.org/abs/1511.05952][T. Schaul et. al.]], and
is widely used to speed up reinforcement learning (as far as I know).

Roughly speaking, mis-predicted observations will be learned more
frequently. To compensate distorted probability, weight of learning is
scaled to the opposite direction (cf. importance sampling).

In cpprb, ~PrioritizedReplayBuffer~ class implements these
functionalities with proportional base (instead of rank base)
priorities as shown at bellow table.

#+CAPTION: Parameter Definition
| \(i\)-th                | Definition                                                                                                                                                  |
|-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Probability: \( P(i) \) | \( \frac{(P_{i}+\epsilon)^{\alpha}}{\sum _{j=0}^{N} (P_{j}+\epsilon)^{\alpha}} \)                                                                           |
| Weight:  \( w_i \)      | \( \frac{\left( \frac{1}{N} \frac{1}{P(i)}\right) ^{\beta}}{\left( \frac{1}{N} \frac{1}{\min _{j} P(j)}\right) ^{\beta}} \to \left(\frac{\min _{j} P(j)}{P(i)}\right) ^{\beta} \) |

You can ~add~ ~priorities~ together with other environment. If no
~priorities~ are passed, the stored maximum priority is used.


The ~dict~ returned by ~sample~ also has special key-values of
~indexes~ and ~weights~. The ~indexes~ are intended to be passed to
~update_priorities~ to update their priorities after comparison with
new prediction. The ~weights~ can be used to scale errors when
updating network parameters. Since the ~weights~ are normalized by the
largest weight, as the original research did, the values are always
less than or equal to 1.


~PrioritizedReplayBuffer~ has hyperparameters ~alpha~ (\( \alpha \))
and ~eps~ (\( \epsilon \)) at constructor and ~beta~ (\( \beta \)) at
~sample~ method. Their default values are ~0.6~, ~1e-4~, and ~0.4~,
respectively. The detail is described in the original paper above.


#+CAPTION: Nstep Parameters
| Parameters             | Default       | Description                                                                   |
|------------------------+---------------+-------------------------------------------------------------------------------|
| ~alpha~ (\( \alpha \)) | \( 0.6 \)     | Prioritized parameter. ~0~ means uniform sampling.                            |
| ~beta~ (\( \beta \))   | \( 0.4 \)     | Compensation parameter. ~1~ means fully compensate (i.e. Importance Sampling) |
| ~eps~ (\( \epsilon \)) | \( 10^{-4} \) | Small value to avoid ~0~ priority.                                            |


*** Example Usage
#+begin_src python
import numpy as np
from cpprb import PrioritizedReplayBuffer

buffer_size = 256

prb = PrioritizedReplayBuffer(buffer_size,
                              {"obs": {"shape": (4,4)},
                               "act": {"shape": 3},
                               "rew": {},
                               "next_obs": {"shape": (4,4)},
                               "done": {}},
                              alpha=0.5)

for i in range(1000):
    prb.add(obs=np.zeros((4,4)),
            act=np.ones(3),
            rew=0.5,
            next_obs=np.zeros((4,4)),
            done=0)

batch_size = 32
s = prb.sample(batch_size,beta=0.5)

indexes = s["indexes"]
weights = s["weights"]

#  train
#  ...

new_priorities = np.ones_like(weights)
prb.update_priorities(indexes,new_priorities)
#+end_src
*** Notes
The constructor of ~PrioritizedReplayBuffer~ (aka. ~__init__~) has
parameter ~check_for_update~. If the parameter is ~True~ (default is
~False~), the buffer traces updated indices after the last calling of
~sample()~ method and the ~update_priorities()~ method updates
priorities only at unchanged indices. This feature is designed for
multiprocess learning in order to avoid mis-updating priorities of
already overwritten values. (This protection is always enabled at
~MPPrioritizedReplayBuffer~)

*** Technical Detail
To choose prioritized sample efficiently, partial summation and
minimum of pre-calculated weights are stored in Segment Tree data
structure, which is written by C++ and which was an initial main
motivation of this project.

To support multiprocessing, the Segment Tree can be lazily updated,
too. (~MPPrioritizedReplayBuffer~)

Minibatch sampling is done by stratified sampling, which samples from
every equal probability space, resulting smaller distributions among
minibatches.


** DONE Nstep Experience Replay
CLOSED: [2019-11-10 Sun 13:46]
:PROPERTIES:
:EXPORT_FILE_NAME: nstep
:END:
*** Overview

To reduce fluctuation of random sampling effect especially at
bootstrap phase, N-step reward (discounted summation) are useful. By
expanding Bellman equation, a N-step target of Q function becomes
$\sum _{k=0}^{N-1} \gamma ^k r_{t+k} + \gamma ^N \max _{a}
Q(s_{t+N},a)$.

According to [[https://arxiv.org/abs/2007.06700][W. Fedus et. al.]], N-step reward can utilize larger buffer
more effectively. Even though theoretically N-step reward, which is
based on a policy at exploration, is not justified for off-policy, it
still works better.

You can create N-step version replay buffer by specifying ~Nstep~
parameter at constructors of ~ReplayBuffer~ or
~PrioritizedReplayBuffer~. Without modification of its environment,
cpprb summarizes N-step rewards and slides "next" values like ~next_obs~.

~Nstep~ parameter is a ~dict~ with keys of ~"size"~ , ~"rew"~,
~"gamma"~ , and ~"next"~ . ~Nstep["size"]~ is a N-step size and 1-step
is identical with ordinary replay buffer (but
inefficient). ~Nstep["rew"]~, whose type is ~str~ or array-like of
~str~, specifies the (set of) reward(s). ~Nstep["gamma"]~ is a
discount factor for reward summation.  ~Nstep["next"]~ , whose type is
~str~ or array like of ~str~, specifies the (set of) next type
value(s), then ~sample~ method returns (i+N)-th value instead of
(i+1)-th one.


~sample~ also adds ~"discounts"~ (\( \gamma ^{N-1} \)) into returned ~dict~.


Since N-step buffer temporary store the values into local storage, you
need to call ~on_episode_end~ member function at the end of the every
episode end to flush into main storage properly.

| Parameters | Type                         | Description                |
|------------+------------------------------+----------------------------|
| ~size~     | ~int~                        | Nstep size                 |
| ~rew~      | ~str~ or array-like of ~str~ | Nstep reward               |
| ~gamma~    | ~float~                      | Discount factor            |
| ~next~     | ~str~ or array-like of ~str~ | Next items (e.g. next_obs) |


*** Example Usage
#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

rb = ReplayBuffer(32,{'obs': {"shape": (4,4)},
                      'act': {"shape": 3},
                      'rew': {},
                      'next_obs': {"shape": (4,4)},
                      'done': {}},
                  Nstep={"size": 4,
                         "gamma": 0.99,
                         "rew": "rew",
                         "next": "next_obs"})

for i in range(100):
    done = 1.0 if i%10 == 9 else 0.0
    rb.add(obs=np.zeros((4,4)),
           act=np.ones((3)),
           rew=1.0,
           next_obs=np.zeros((4,4)),
           done=0.0)
    if done:
        rb.on_episode_end()

rb.sample(16)
#+end_src

*** Notes
This N-step feature assumes sequential transitions in a trajectory
(episode) are stored sequentially. If you utilize distributed agent
configuration, you must add a single episode simultaneously.


*** Technical Detail

** DONE Memory Compression
CLOSED: [2019-11-10 Sun 13:33]
:PROPERTIES:
:EXPORT_FILE_NAME: memory_compression
:END:

Since replay buffer stores a large number of data set, memory
efficiency is one of the most important point.

In cpprb, there are two *optional* functionalities named ~next_of~ and
~stack_compress~, which you can turn on manually when constructing
replay buffer.

~next_of~ and ~stack_compress~ can be used together, but currently
none of them are compatible with N-step replay buffer.

These memory compressions rely on the internal memory alignment, so
that these functionalities cannot be used in situations where
sequential steps are not stored sequentially (e.g. distributed
reinforcement learning).

*** ~next_of~

**** Overview
In reinforcement learning, usually a set of observations before and
after a certain action are used for training, so that you save the set
in your replay buffer together. Naively speaking, all observations are
stored twice.

As you know, replay buffer is a ring buffer and the next value should
be stored at the next index, except for the newest edge.

If you specify ~next_of~ argument (whose type is ~str~ or array like
of ~str~), the "next value" of specified values are also created in
the replay buffer automatically and they share the memory location.

The name of the next value adds prefix ~next_~ to the original name
(e.g. ~next_obs~ for ~obs~, ~next_rew~ for ~rew~, and so on).

This functionality has small penalties for manipulating sampled index
and checking the cache for the newest index. (As far as I know, this
penalty is not significant, and you might not notice.)

**** Example Usage
#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

buffer_size = 256

rb = ReplayBuffer(buffer_size,{"obs": {"shape": (84,84)},
                               "act": {"shape": 3},
                               "rew": {},
                               "done": {}}, # You must not specify "next_obs" nor "next_rew".
                  next_of=("obs","rew"))

rb.add(obs=np.ones((84,84)),
       act=np.ones(3),
       next_obs=np.ones((84,84)),
       rew=1,
       next_rew=1,
       done=0)
#+end_src

**** Notes
cpprb does not check the consistence of i-th ~next_foo~ and (i+1)-th
~foo~. This is user responsibility.

Since ~next_foo~ is automatically generated, you must not specify it
in the constructor manually.

**** Technical Detail
Internally, ~next_foo~ is not stored into a ring buffer, but into its chache.
(So still raising error if you don't pass them to ~add~.)

When sampling the ~next_foo~, indices (which is ~numpy.ndarray~) are
shifted (and wraparounded if necessary), then are checked whether they
are on the newest edge of the ring buffer. If the indices are on the
edge, the cached one is extracted.

*** ~stack_compress~

**** Overview
~stack_compress~ is designed for compressing stacked (or sliding
windowed) observation. A famous use case is Atari video game, where 4
frames of display windows are treated as a single observation and the
next observation is the one slided by only 1 frame
(e.g. 1,2,3,4-frames, 2,3,4,5-frames, 3,4,5,6-frames, ...). For this
example, a straight forward approach stores all the frames 4 times.

cpprb with ~stack_compress~ does not store duplicated frames in
stacked observation (except for the end edge of the internal ring
buffer) by utilizing numpy sliding trick.

You can specify ~stack_compress~ parameter, whose type is ~str~ or
array like of ~str~, at constructor.

**** Sample Usage
The following sample code stores ~4~-stacked frames of ~16x16~ data as
a single observation.

#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

rb = ReplayBuffer(32,{"obs":{"shape": (16,16,4)}, 'rew': {}, 'done': {}},
                  next_of = "obs", stack_compress = "obs")

rb.add(obs=(np.ones((16,16,4))),
       next_obs=(np.ones((16,16,4))),
       rew=1,
       done=0)
#+end_src
**** Notes
In order to make compatible with [[https://github.com/openai/gym][OpenAI gym]], the last dimension is
considered as stack dimension (which is not fit to C array memory
order).

For the sake of performance, cpprb does not check the overlapped data
are truly identical, but simply overwrites with new data. Users must
not specify ~stack_compress~ for non-stacked data.

**** Technical Detail
Technically speaking ~numpy.ndarray~ (and other data type supporting
buffer protocol) has properties of item data type, the number of
dimensions, length of each dimension, memory step size of each
dimension, and so on. Usually, no data should overlap memory address,
however, ~stack_compress~ intentionally overlaps the memory addresses
in the stacked dimension.



** DONE Map Large Data on File
CLOSED: [2020-08-02 Sun 18:00]
:PROPERTIES:
:EXPORT_FILE_NAME: mmap
:END:

From version 9.2, ~ReplayBuffer~ accepts ~mmap_prefix~ keyword at
constructor. If ~mmap_prefix~ is specified, internal memory data are
mapped on files by using mmap.

This feature is useful when a required memory size is larger than
physical RAM size. Since the data is stored on file directly, this
might be slower.

The file names are ~f"{mmap_prefix}_{name}.dat"~.

** DONE Multiprocess Learning (Ape-X)
CLOSED: [2021-01-11 Mon 08:01]
:PROPERTIES:
:EXPORT_FILE_NAME: ape-x
:END:

From version 9.4.2, new classes ~MPReplayBuffer~ and
~MPPrioritizedReplayBuffer~ support multiprocess learning like [[https://arxiv.org/abs/1803.00933][Ape-X]]
(single learner with multiple explorers) on single machine
efficiently.

*** Shared Memory
First of all, ~MPReplayBuffer~ and ~MPPrioritizedReplayBuffer~ maps
internal data on shared memory. This means you don't need to use proxy
(e.g. ~multiprocessing.managers.SyncManager~) or queue
(e.g. ~multiprocessing.Queue~) for interproecss data sharing, but you
can simply access the buffer object from different process.

#+begin_src python
from multiprocessing import Process
from cpprb import MPPrioritizedReplayBuffer

rb = MPPrioritizedReplayBuffer(100,{"obs": {},"done" {}})

def explorer(rb):
    for _ in range(100):
        # Something ...
        rb.add(obs=obs, done=done)

p = Process(target=explorer,args=[rb]) # You can pass to Process simply as argument
p.start()
p.join()

sample = p.sample(10) # You can access data stored at different process.
#+end_src

*** Efficient Lock
Although you can implement Ape-X with ordinary =ReplayBuffer= or
=PrioritizedReplayBuffer= class, locking entire buffer when writing
and reading is quite inefficient.

#+begin_src python
# Part of Explorer Naiive Implementation
if local_buffer.get_stored_size() > local_size:
    local_sample = local_buffer.get_all_transitions()
    local_buffer.clear()

    with lock: # Inefficient: Lock entire buffer during addition
        global_buffer.add(**local_sample)
#+end_src

~MPReplayBuffer~ and ~MPPrioritizedReplayBuffer~ automatically lock
only critical section instead of entire buffer. For example, since
sequential ~add~ method calls should write different memory address,
its critical section is only index fetching and increment. This
locking reduction allows multiple explorers to add transitions
parallelly.[fn:1]

[[/cpprb/images/apex-per.png]]
*** Limitation
~MPReplayBuffer~ and ~MPPrioritizedReplayBuffer~ don't support
features of [[https://ymd_h.gitlab.io/cpprb/features/nstep/][Nstep Experience Replay]], [[https://ymd_h.gitlab.io/cpprb/features/memory_compression/][Memory Compression]], and [[https://ymd_h.gitlab.io/cpprb/features/mmap/][Map Data
on File]]. (You can still utilize these features at local buffers of
explorers.)

~MPReplayBuffer~ and ~MPPrioritizedReplayBuffer~ assume single
learner (~sample~ / ~update_priorities~) and multiple explorers
(~add~). You must not call learner functions from multiple processes
simultaneously.

*** Example Code
#+INCLUDE: "../example/apex.py" src python

* Contributing
:PROPERTIES:
:EXPORT_HUGO_SECTION*: contributing
:END:

** DONE Step by Step Merge Request
CLOSED: [2020-01-17 Fri 23:09]
:PROPERTIES:
:EXPORT_FILE_NAME: merge_request
:END:

The first step of coding contribution is to fork cpprb on GitLab.com.

The detail steps for fork is described at [[https://docs.gitlab.com/ee/gitlab-basics/fork-project.html][official document]].

After fork cpprb on the web, you can clone repository to your local
machine and set original cpprb as "upstream" by

#+begin_src shell
git clone https://gitlab.com/<Your GitLab Account>/cpprb.git
cd cpprb
git remote add upstream https://gitlab.com/ymd_h/cpprb.git
#+end_src

To make "master" branch clean, you need to create new branch before you edit.

#+begin_src shell
git checkout -b <New Branch Name> master
#+end_src

This process is necessay because "master" and other original branches
might progress during your working.


From here, you can edit codes and make commit as usual.


After finish your work, you must recheck original cpprb and ensure
there is no cnflict.

#+begin_src shell
git pull upstream master
git checkout <Your Branch Name>
git merge master # Fix confliction here!
#+end_src


If everything is fine, you push to your cpprb.

#+begin_src shell
git push origin <Your Branch Name>
#+end_src

Merge request can be created from the web, the detail is described at
[[https://docs.gitlab.com/ee/user/project/merge_requests/creating_merge_requests.html][official document]].


There is [[https://stackoverflow.com/a/14681796][a good explanation]] for making good Pull Request (merge
request equivalent on GitHub.com)

* DONE Examples
CLOSED: [2020-02-15 Sat 09:23]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: examples
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 800
:END:

** Multiprocess Learning (Ape-X) with ~MPPrioritizedReplayBuffer~
See [[https://ymd_h.gitlab.io/cpprb/features/ape-x/][Multiprocess Learning (Ape-X)]] for detail.

#+INCLUDE: "../example/apex.py" src python

** Create ~ReplayBuffer~ for non-simple =gym.Env= with helper functions

#+INCLUDE: "../example/create_buffer_with_helper_func.py" src python

* Comparison
:PROPERTIES:
:EXPORT_HUGO_SECTION*: comparison
:EXPORT_HUGO_WEIGHT: 850
:END:

** DONE Comparison
CLOSED: [2020-02-16 Sun 23:08]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:END:

In this section, we compare cpprb with other replay buffer implementations;

- [[https://github.com/openai/baselines][OpenAI Baselines]]
  - [[https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py][baselines.deepq.replay_buffer.ReplayBuffer]]
  - [[https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py][baselines.deepq.replay_buffer.PrioritizedReplayBuffer]]
- [[https://github.com/ray-project/ray][Ray RLlib]]
  - [[https://github.com/ray-project/ray/blob/master/rllib/execution/replay_buffer.py][ray.rilib.execution.replay_buffer.ReplayBuffer]]
  - [[https://github.com/ray-project/ray/blob/master/rllib/execution/replay_buffer.py][ray.rllib.execution.replay_buffer.PrioritizedReplayBuffer]]
- [[https://github.com/chainer/chainerrl][Chainer ChainerRL]]
  - [[https://github.com/chainer/chainerrl/blob/master/chainerrl/replay_buffers/replay_buffer.py][chainerrl.replay_buffers.ReplayBuffer]]
  - [[https://github.com/chainer/chainerrl/blob/master/chainerrl/replay_buffers/prioritized.py][chainerrl.replay_buffers.PrioritizedReplayBuffer]]
- [[https://github.com/deepmind/reverb][DeepMind Reverb]]


*Important Notice*

Except cpprb and DeepMind/Reverb, replay buffers are only a part of
their reinforcement learning ecosystem. These libraries don't focus on
providing greatest replay buffers but reinforcement learning.

Our motivation is to provide strong replay buffers to researchers and
developers who not only use existing networks and/or algorithms but
also creating brand-new networks and/or algorithms.

Here, we would like to show that cpprb is enough functional and enough
efficient compared with others.

*** OpenAI Baselines
[[https://github.com/openai/baselines][OpenAI Baselines]] is a set of baseline implementations of reinforcement
learning developed by OpenAI.

The source code is published with MIT license.

Ordinary and prioritized experience replay are implemented with
~ReplayBuffer~ and ~PrioritizedReplayBuffer~ classes, respectively.
Using these classes directly is (probably) not expected, but you can
import them like this;

#+begin_src python
from baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer
#+end_src

~ReplayBuffer~ is initialized with ~size~ parameter for replay buffer
size. Additionally, ~PrioritizedReplayBuffer~ requires ~alpha~
parameter for degree of prioritization, too. These parameters doesn't
have default values, so that you need to specify them.

#+begin_src python
buffer_size = int(1e6)
alpha = 0.6

rb = ReplayBuffer(buffer_size)
prb = PrioritizedReplayBuffer(buffer_size,alpha)
#+end_src

A transition is stored into replay buffer by calling
~ReplayBuffer.add(self,obs_t,action,reward,obs_tp1,done)~.

For ~PrioritizedReplayBuffer~, the maximum priority at that time is
automatically used for a newly added transition.

These replay buffers are ring buffers, so that the oldest transition
is overwritten by a new transition after the buffer becomes full.

#+begin_src python
obs_t = [0, 0, 0]
action = [1]
reward = 0.5
obs_tp1 = [1, 1, 1]
done = 0.0

rb.add(obs_t,action,reward,obs_tp1,done)
prb.add(obs_t,action,reward,obs_tp1,done) # Store with max. priority
#+end_src

Stored transitions can be sampled by calling
~ReplayBuffer.sample(self,batch_size)~ or
~PrioritizedReplayBuffer.sample(self,batch_size,beta)~.

~ReplayBuffer~ returns a tuple of batch size
transition. ~PrioritizedReplayBuffer~ also returns weights and
indexes, too.

#+begin_src python
batch_size = 32
beta = 0.4

obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = rb.sample(batch_size)
obs_batch, act_batch, rew_batch, next_obs_batch, done_mask, weights, idxes = prb.sample(batch_size)
#+end_src

Priorities can be updated by calling
~PrioritizedReplayBuffer.update_priorities(self,idxes,priorities)~.

#+begin_src python
prb.update_priorities(idxes,priorities)
#+end_src

Internally, these replay buffers utilize Python ~list~ for storage, so
that the memory usage gradually increase until the buffer becomes
full.

*** Ray RLlib
[[https://docs.ray.io/en/latest/rllib.html][RLlib]] is reinforcement learning library based on distributed framework
[[https://github.com/ray-project/ray][Ray]].

The source code is published with Apache-2.0 license.

Ordinary and prioritized experience replay are implemented with
~ReplayBuffer~ and ~PrioritizedReplayBuffer~ classes, respectively.

These classes are decorated with ~@DeveloperAPI~, which are intented
to be used by developer when making custom algorithms.

#+begin_src python
from ray.rllib.execution.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer
#+end_src

These replay buffer classes initialize like OpenAI Baselines;

#+begin_src python
buffer_size = int(1e6)
alpha = 0.6

rb = ReplayBuffer(buffer_size)
prb = PrioritizedReplayBuffer(buffer_size,alpha)
#+end_src

A transition is stored by calling ~ReplayBuffer.add(self,item,weight)~
and ~PrioritizedReplayBuffer.add(self,item,weight)~. The ~item~ is a
instance of ~ray.rllib.policy.sample_batch.SampleBatch~. (The API was
changed at [[https://github.com/ray-project/ray/releases/tag/ray-0.8.7][ray-0.8.7]].)

The ~SampleBatch~ class can have any kind of values with batch size,
however, only a single transition is allowed for addition. (If you
pass ~SampleBatch~ having multi-transitions to ~add()~ method, it will
produce unintentional bug probably.) The key of ~SampleBatch~ can be
defined by user freely.

In RLlib, ~PrioritizedReplayBuffer~ can take ~weight~ parameter to
specify priority at the same time. In order to unify their API,
~ReplayBuffer~ also requires ~weight~ parameter, even though it is not
used at all. Moreover, the ~weight~ does not have default parameter
(such as ~None~), you need to pass something explicitly.

#+begin_src python
obs_t = [0, 0, 0]
action = [1]
reward = 0.5
obs_tp1 = [1, 1, 1]
done = 0.0
weight = 0.5

# For addition, SampleBatch must be initialized with a set of `list` having only a single element.
rb.add(SampleBatch(obs=[obs_t],
                   action=[action],
                   reward=[reward],
                   new_obs=[obs_tp1],
                   done=[done]),None)
prb.add(SampleBatch(obs=[obs_t],
                    action=[action],
                    reward=[reward],
                    new_obs=[obs_tp1],
                    done=[done]),weight)
#+end_src


Like OpenAI Baselines, stored transitions can be sampled by calling
~ReplayBuffer.sample(self,batch_size)~ or
~PrioritizedReplayBuffer.sample(self,batch_size,beta)~.

~ReplayBuffer~ returns ~SampleBatch~ of batch size transition. (The
API was changed at [[https://github.com/ray-project/ray/releases/tag/ray-0.8.7][ray-0.8.7]].) ~PrioritizedReplayBuffer~ also returns
~weights~ and ~batch_indexes~ in ~SampleBatch~, too.

#+begin_src python
batch_size = 32
beta = 0.4

sample = rb.sample(batch_size)
obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = sample["obs"], sample["action"], sample["reward"], sample["new_obs"], sample["done"]

sample = prb.sample(batch_size)
obs_batch, act_batch, rew_batch, next_obs_batch, done_mask, weights, idxes = sample["obs"], sample["action"], sample["reward"], sample["new_obs"], sample["done"], sample["weights"], sample["batch_indexes"]
#+end_src

Priorities can be also updated by calling
~PrioritizedReplayBuffer.update_priorities(self,idxes,priorities)~,
too.

#+begin_src python
prb.update_priorities(idxes,priorities)
#+end_src

Internally, these replay buffers utilize Python ~list~ for storage, so
that the memory usage gradually increase until the buffer becomes
full.

*** Chainer ChainerRL
[[https://github.com/chainer/chainerrl][ChainerRL]] is a deep reinforcement learning library based on a
framework [[https://github.com/chainer/chainer][Chainer]]. Chainer (including ChainerRL) has already stopped
active development, and development team (Preferred Networks) joined
to [[https://pytorch.org/][PyTorch]] development.

The source code is published with MIT license.

Ordinary and prioritized experience replay are implemented with
~ReplayBuffer~ and ~PrioritizedReplayBuffer~, respectively.

#+begin_src python
from chainerrl.replay_buffers import ReplayBuffer, PrioritizedReplayBuffer
#+end_src

ChainerRL has slighly different API from OpenAI Baselines' one.

~ReplayBuffer~ is initialized with ~capacity=None~ parameter for
buffer size and ~num_steps=1~ parameter for Nstep configuration.

~PrioritizedReplayBuffer~ can additionally take parameters of
~alpha=0.6~, ~beta0=0.4~, ~betasteps=2e5~, ~eps=0.01~,
~normalize_by_max=True~, ~error_min=0~, ~error_max=1~.

In ChainerRL, beta (correction for weight) parameter starts from
~beta0~, automatically increases with equal step size during
~betasteps~ time iterations and after that becomes ~1.0~.

#+begin_src python
buffer_size = int(1e6)
alpha = 0.6

rb = ReplayBuffer(buffer_size)
prb = PrioritizedReplayBuffer(buffer_size,alpha)
#+end_src

A transition is stored by calling
~ReplayBuffer.append(self,state,action,reward,next_state=None,next_action=None,is_state_terminate=False,env_id=0,**kwargs)~.
Additional keyward arguments are stored, too, so that you can use any
custom environment values. By specifying ~env_id~, multiple trajectory
can be tracked with Nstep configuration.

#+begin_src python
obs_t = [0, 0, 0]
action = [1]
reward = 0.5
obs_tp1 = [1, 1, 1]
done = False

rb.add(obs_t,action,reward,obs_tp1,is_state_terminal=done)
prb.add(obs_t,action,reward,obs_tp1,is_state_terminal=done)
#+end_src

Stored transitions are sampled by calling
~ReplayBuffer.sample(self,num_experience)~ and
~PrioritizedReplayBuffer.sample(self,n)~.

Apart from other implementations, ChainerRL's replay buffers return
unique (non-duplicated) transitions, so that batch_size must be
smaller than stored transition size. Furthermore, they return a Python
~list~ of a ~dict~ of transition instead of a Python ~tuple~ of
environment values.

#+begin_src python
batch_size = 32

# Need additional modification to take apart
transitions = rb.sample(batch_size)
transitions_with_weight = prb.sample(batch_size)
#+end_src

Update index cannot be specified manually, but
~PrioritizedReplayBuffer~ memorizes sampled indexes.
(Without ~sample~, user cannot update priority.)

#+begin_src python
prb.update_priorities(priorities)
#+end_src


Internally, these replay buffers utilize Python ~list~ for storage, so
that the memory usage gradually increase until the buffer becomes
full. In ChainerRL, storage is not a simple Python ~list~, but two
~list~ to pop out the oldest element with O(1) time.

*** DeepMind Reverb
[[https://github.com/deepmind/reverb][Reverb]] is relatively new, which was released on 26th May 2020 by
DeepMind.

Reverb is a framework for experience replay like cpprb. By utilizing
server-client model, Reverb is mainly optimized for large-scale
distributed reinforcement learning.

The source code is published with Apache-2.0 license.

Currently (28th June 2020), Reverb officially says it is still
non-production level and requries a development version of TensorFlow
(i.e. tf-nightly 2.3.0.dev20200604).

Ordinary and prioritized experience replay are constructed by setting
~reverb.selectors.Uniform()~ and ~reverb.selectors.Prioritized(alpha)~
to ~sampler~ argument in ~reverb.Table~ constructor, respectively.

Following sample code constructs a server with two replay buffers
listening a port ~8000~.

#+begin_src python
import reverb

buffer_size = int(1e6)
alpha = 0.6

server = reverb.Server(tables=[reverb.Table(name="ReplayBuffer",
                                            sampler=reverb.selectors.Uniform(),
                                            remover=reverb.selectors.Fifo(),
                                            rate_limiter=reverb.rate_limiters.MinSize(1),
                                            max_size=buffer_size),
                               reverb.Table(name="PrioritizedReplayBuffer",
                                            sampler=reverb.selectors.Prioritized(alpha),
                                            remover=reverb.selectors.Fifo(),
                                            rate_limiter=reverb.rate_limiters.MinSize(1),
                                            max_size=buffer_size)],
                       port=8000)
#+end_src

By changing ~selector~ and ~remover~, we can use different algorithms
for sampling and overwriting, respectively.

Supported algorithms implemented at ~reverb.selectors~ are following;

- ~Uniform~: Select uniformly.
- ~Prioritized~: Select proportional to stored priorities.
- ~Fifo~: Select oldest data.
- ~Lifo~: Select newest data.
- ~MinHeap~: Select data with lowest priority.
- ~MaxHeap~: Select data with highest priority.


There are 3 ways to store a transition.

The first method uses ~reverb.Client.insert~. Not only prioritized
replay buffer but also ordinary replay buffer requires priority even
though it is not used.

#+begin_src python
import reverb

client = reverb.Client(f"localhost:{server.port}")

obs_t = [0, 0, 0]
action = [1]
reward = [0.5]
obs_tp1 = [1, 1, 1]
done = [0]

client.insert([obs_t,action,reward,obs_tp1,done],priorities={"ReplayBuffer":1.0})
client.insert([obs_t,action,reward,obs_tp1,done],priorities={"PrioritizedReplayBuffer":1.0})
#+end_src

The second method uses ~reverb.Client.writer~, which is internally
used in ~reverb.Client.insert~, too. This method can be more efficient
because you can flush multiple items together by calling
~reverb.Writer.close~ insead of one by one.

#+begin_src python
import reverb

client = reverb.Client(f"localhost:{server.port}")

obs_t = [0, 0, 0]
action = [1]
reward = [0.5]
obs_tp1 = [1, 1, 1]
done = [0]

with client.writer(max_sequence_length=1) as writer:
    writer.append([obs_t,action,reward,obs_tp1,done])
    writer.create_item(table="ReplayBuffer",num_timesteps=1,priority=1.0)

    writer.append([obs_t,action,reward,obs_tp1,done])
    writer.create_item(table="PrioritizedReplayBuffer",num_timesteps=1,priority=1.0)
#+end_src

The last method uses ~reverb.TFClient.insert~. This class is designed
to be used in TensorFlow graph.

#+begin_src python
import reverb

tf_client = reverb.TFClient(f"localhost:{server.port}")

obs_t = tf.constant([0, 0, 0])
action = tf.constant([1])
reward = tf.constant([0.5])
obs_tp1 = tf.constant([1, 1, 1])
done = tf.constant([0])

tf_client.insert([obs_t,action,reward,obs_tp1,done],
                 tables=tf.constant(["ReplayBuffer"]),
                 priorities=tf.constant([1.0],dtype=tf.float64))
tf_client.insert([obs_t,action,reward,obs_tp1,done],
                 tables=tf.constant(["PrioritizedReplayBuffer"]),
                 priorities=tf.constant([1.0],dtype=tf.float64))
#+end_src

~tables~ parameter must be ~tf.Tensor~ of ~str~ with rank 1 and
~priorities~ parameter must be ~tf.Tensor~ of ~float64~ with
rank 1. The lengths of ~tables~ and ~priorities~ must match.


Sampling transitions can be realized by 3 ways, too.

The first method utilizes ~reverb.Client.sample~, which returns
generator of ~reverb.replay_sample.ReplaySample~. As long as we can
investigate, beta-parameter is not supported and weight is not
calculated at prioritized experience replay.

#+begin_src python
batch_size = 32

transitions = client.sample("ReplayBuffer",num_samples=batch_size)
transitions_with_priority = client.sample("PrioritizedReplayBuffer",num_samples=batch_size)
#+end_src


The second method uses ~reverb.TFClient.sample~, which does not
support batch sampling.

#+begin_src python
transition = tf_client.sample("ReplayBuffer",
                              [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64])
transition_priority = tf_client.sample("PrioritizedReplayBuffer",
                                       [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64])
#+end_src


The last method is completely different from others, which calls
~reverb.TFClient.dataset~, returning ~reverb.ReplayDataset~ derived
from ~tf.data.Dataset~.

Once creating ~ReplayDataset~, the dataset can be used as generator
and automatically fetches transitions from its replay buffer with
proper timing.


#+begin_src python
dataset = tf_client.dataset("ReplayBuffer",
                            [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64],
                            [4,1,1,4,1])
dataset_priority = tf_client.dataset("PrioritizedReplayBuffer",
                                     [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64],
                                     [4,1,1,4,1])
#+end_src


Priorities can be updated by ~reverb.Client.mutate_priorities~ or
~reverb.TFClient.update_priorities~. Aside from other implementations,
key is not integer sequence but hash, so that the key must be taken
from sampled items by accsessing ~ReplaySample.info.key~.

#+begin_src python
for t in transitions_with_priority:
    client.mutate_priorities("PriorotizedReplayBuffer",updates={t.info.key: 0.5})

tf_client.update_priorities("PrioritizedReplayBuffer",
                            transition_priority.info.key,
                            priorities=tf.constant([0.5],dtype=tf.float64)
#+end_src

*** Other Implementations
There are also some other replay buffer implementations, which we
couldn't review well. In future, we would like to investigate these
implementations and compare with cpprb.

- [[https://github.com/tensorflow/agents][TF-Agents]] :: TensorFlow official reinforcement learning library
- [[https://github.com/google-research/seed_rl][SEED RL]] :: Scalable and Effificient Deep-RL
- [[https://github.com/tensorflow/models][TensorFlow Model Garden]] :: TensorFlow example implementation


** DONE Functionality
CLOSED: [2020-02-24 Mon 12:49]
:PROPERTIES:
:EXPORT_FILE_NAME: functionality
:EXPORT_HUGO_WEIGHT: 10
:END:

The following table summarizes functionalities of replay buffers.

|                           | cpprb                                 | OpenAI/Baselines                          | Ray/RLlib                                                  | Chainer/ChainerRL          | DeepMind/Reverb |
|---------------------------+---------------------------------------+-------------------------------------------+------------------------------------------------------------+----------------------------+-----------------|
| *Flexible Environment*    | Yes                                   | No                                        | No                                                         | Yes                        | Yes             |
| *Nstep*                   | Yes                                   | No                                        | Yes                                                        | Yes                        | No              |
| *Parellel Exploration*    | Yes (Support Ape-X on single machine) | Yes (Avarages gradients of MPI processes) | Yes (Concatenates sample batches from distributed buffers) | No                         | Yes             |
| *Save/Load*               | No (Cannot =pickle=)                  | No (Maybe can =pickle=)                   | No (Maybe can =pickle=. Trained policies can save/load.)   | Yes                        | Yes             |
| *Deep Learning Framework* | Anything                              | TensorFlow 1.14 (only this version)       | Anything (Helper functions for [[https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow][TensorFlow]] and [[https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-pytorch][PyTorch]])     | [[https://chainer.org/][Chainer]] ([[https://chainer.org/announcement/2019/12/05/released-v7.html][maintenance only]]) | TensorFlow 2.3  |


** DONE Benchmark
CLOSED: [2020-02-16 Sun 23:24]
:PROPERTIES:
:EXPORT_HUGO_WEIGHT: 20
:EXPORT_FILE_NAME: benchmark
:END:

Because of dependant TensorFlow version incompatibility, two set of
benchmarks are executed.

*** Benchmark 1
The first benchmark compares OpenAI/Baselines, Ray/RLlib,
Chainer/ChainerRL, and cpprb.

**** Settings

We use following docker image to take benchmarks;

#+INCLUDE: "../benchmark/Dockerfile" src dockerfile

- OpenAI Baselines requires TensorFlow 1.14
- OpenAI Baselines at PyPI seems to be obsolete and requires non-free MuJoCo.
- RLlib requres Pandas, too.


The benchmark script is as follows;

#+INCLUDE: "../benchmark/benchmark.py" src python


**** Results
[[/cpprb/benchmark/ReplayBuffer_add.png]]

[[/cpprb/benchmark/ReplayBuffer_sample.png]]

[[/cpprb/benchmark/PrioritizedReplayBuffer_add.png]]


[[/cpprb/benchmark/PrioritizedReplayBuffer_sample.png]]


*** Benchmark 2
The second benchmark compares DeepMind/Reverb and cpprb.
Reverb has multiple ways of adding and sampling.

**** Settings
We use following docker image to take benchmarks;

#+INCLUDE: "../benchmark2/Dockerfile" src dockerfile

- DeepMind/Reverb requires development version TensorFlow 2.3.0.dev20200604

The benchmark script is as follows;

#+INCLUDE: "../benchmark2/benchmark.py" src python

**** Results
[[/cpprb/benchmark/ReplayBuffer_add2.png]]

[[/cpprb/benchmark/ReplayBuffer_sample2.png]]

[[/cpprb/benchmark/PrioritizedReplayBuffer_add2.png]]


[[/cpprb/benchmark/PrioritizedReplayBuffer_sample2.png]]

* DONE Misc
CLOSED: [2020-01-17 Fri 22:31]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: misc
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 999
:END:

In this section, cpprb related miscellaneous information are described.

- [[https://ymd_h.gitlab.io/cpprb/misc/links/][Links]]
- [[https://ymd_h.gitlab.io/cpprb/misc/lisence/][License]]
- [[https://ymd_h.gitlab.io/cpprb/misc/logo/][Logo]]

* DONE Logo
CLOSED: [2020-08-04 Tue 09:19]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: misc
:EXPORT_FILE_NAME: logo
:END:

[[https://ymd_h.gitlab.io/cpprb/images/logo.png]]

The cpprb logo was degined by the author (H. Yamada) at the beginning
of 2020. The logo comes from the package name "cpprb". The first "c"
letter imitates "replay" button, indicating the key feature of
experience replay. The "r" letter represents reward in formula
(e.g. Bellman equation), which is another key concept of reinforcement
learning.

* DONE FAQ
CLOSED: [2020-06-06 Sat 13:50]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION*: faq
:EXPORT_HUGO_WEIGHT: 900
:END:

** Can I save/load ReplayBuffer?

No. Unfortunately, save/load full ~ReplayBuffer~ object state is not
supported.

However, you can get all the stored transisions (without "priority" in
~PrioritizedReplayBuffer~) by calling
~ReplayBuffer.get_all_transitions()~ and a set of transitions can be
stored at the same time by calling ~ReplayBuffer.add(**kwargs)~.

** "IlIegal instruction (core dumped)" when "import cpprb"

Pre-built binary does not match with your environment. Please
re-install from source code.

#+begin_src shell
pip uninstall cpprb
pip install cpprb --no-binary
#+end_src

This installation requires sufficient C++ compiler. (see [[https://ymd_h.gitlab.io/cpprb/installation/][Installation]])

* DONE Survey
CLOSED: [2021-01-23 Sat 09:18]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 950
:EXPORT_HUGO_SECTION*: survey
:END:

In this section, we try to survey papers related to experience replay.

Some of them can be implemented in cpprb in future. Some can be just
know-how of experience replay.

If you have any ideas or requests, please post to [[https://github.com/ymd-h/cpprb/discussions][GitHub Disccusions]].


- [[https://ymd_h.gitlab.io/cpprb/survey/ero/][Experience Replay Optimization (ERO)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/ref-er/][Remember and Forget for Experience Replay (ReF-ER)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/aer/][Attentive Experience Replay (AER)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/cer/][Competitive Experience Replay (CER)]]
- [[https://ymd_h.gitlab.io/cpprb/survey/der/][Dynamic Experience Replay (DER)]]


* DONE Experience Replay Optimization (ERO)
CLOSED: [2021-01-23 Sat 09:28]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: survey
:EXPORT_FILE_NAME: ero
:END:

** Overview
[[https://www.ijcai.org/Proceedings/2019/589][Experience Replay Optimization (ERO)]] masks stored transitions in order
to improve sample efficiency. Additional neural network, "replay
policy", takes features extracted from each transition and infers mask
probability. Agent samples from masked transitions uniformly.

In order to train replay policy, binary masks (\( \mathbf{I} \))
produced by Bernoulli distribution are considered as action. The
replay-reward (\( r^{r} \)) as defined as the difference between
culmutive reward of the previous policy (\( \pi ^{\prime} \)) and that
of agent policy (\( \pi \)); \( r ^{r} = r^{c}_{\pi} - r^{c}_{\pi
^{\prime}} \).

The policy gradient for mini batch can be written as follows;

\[ \sum _{j:B_j \in B^{\text{batch}}} r^r \nabla [ \mathbf{I}_j \log \phi + (1-\mathbf{I}_j) \log (1-\phi) ] \]


** With cpprb
We plan to implement somthing like =BernoulliMaskedReplayBuffer=
to support ERO. Together with such future enhancement, users still
need to implement neural network which infers probabilities of
Bernoulli masks.

** References
- [[https://www.ijcai.org/Proceedings/2019/589][D. Zha et. al., "Experience Replay Optimization", IJCAI (2019) 4243-4249]] ([[https://arxiv.org/abs/1906.08387][arXiv]])

* DONE Remember and Forget for Experience Replay (ReF-ER)
CLOSED: [2021-01-24 Sun 11:39]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: survey
:EXPORT_FILE_NAME: ref-er
:END:

** Overview
[[http://proceedings.mlr.press/v97/novati19a.html][Remember and Forget for Experience Replay (ReF-ER)]] dismisses
transtions from "far policy". The key metrics, importance
(\(\rho_t\)), is the ratio between the probability of selecting
(\(a_t\)) with the current policy (\(\pi ^{w}\)) and with the behavior
policy (\(\mu _t\)); \(\rho _t = \pi (a_t\mid s_t) / \mu _t (a_t\mid s_t)\).

If \(1/c_{\text{max}} < \rho _t < c_{\text{max}}\) then it is classified
as "near-policy", otherwise "far-policy". The gradients
(\(\hat{g}(w)\)) computed from far-policy are clipped to 0.

Additionally, penalty term (\(\hat{g}^D(w)\)) is defined as follows;
\(\hat{g}^D(w)=E[\nabla D_{\text{KL}}(\mu _k ( \cdot \mid s_k)\| \pi ^w ( \cdot \mid s_k))]\).

These two terms are added with annealing parameter \(\beta\);

\[ \hat{g}^{\text{ReF-ER}}(w) = \beta \hat{g}(w) + (1-\beta) \hat{g}^D(w) \]

The \(\beta \) is updated at each step by following rule;

\[ \beta \leftarrow \cases{ (1-\eta )\beta & if \(n_{\text{far}} /N > D\) \cr (1-\eta )\beta + \eta & otherwise} \]

where \(\eta\) is learning rate of neural network, \(N\) is the number
of total samples in the replay buffer, \(n_{\text{far}}\) is the
number of far policy samples in the replay buffer, and \(D\) is a
hyperparameter.

** With cpprb
Under investigation. It is still not clear how do the authors sample
from replay buffer. We continue to investigate [[https://github.com/cselab/smarties][their code]].

** References
- [[http://proceedings.mlr.press/v97/novati19a.html][G. Novati and P. Koumoutsakos, "Remember and Forget for Experience Replay", ICML (2019)]] ([[https://arxiv.org/abs/1807.05827][arXiv]], [[https://github.com/cselab/smarties][code]])

* DONE Attentive Experience Replay (AER)
CLOSED: [2021-01-24 Sun 22:59]
:PROPERTIES:
:EXPORT_FILE_NAME: aer
:EXPORT_HUGO_SECTION*: survey
:END:

** Overview
[[https://ojs.aaai.org//index.php/AAAI/article/view/6049][Attentive Experience Replay (AER)]] uniformly samples \(\lambda\) times
greater size than mini-batch size (\(k\)), then choose top \(k\)
similar samples as minibatch.

The coefficient \(\lambda\) is annealed to 1 during training.

The similarity function \(\mathcal{F}(s_j,s_t)\) is task dependent. In
the paper, the authors used cosine similarity for [[http://www.mujoco.org/][MuJoCo]] and norm of
difference between embedded features for Atari 2600 games.

** With cpprb
You can implement AER with simple =ReplayBuffer= class. Simply you can
sample enlarged batch size then compute similarity.

** References
- [[https://ojs.aaai.org//index.php/AAAI/article/view/6049][P. Sun et. al., "Attentive Experience Replay", AAAI (2020) 34, 5900-5907]]

* DONE Competitive Eeperience Replay (CER)
CLOSED: [2021-01-30 Sat 12:55]
:PROPERTIES:
:EXPORT_FILE_NAME: cer
:EXPORT_HUGO_SECTION*: survey
:END:

** Overview
[[https://iclr.cc/Conferences/2019/Schedule?showEvent=1091][Competitive Experience Replay (CER)]] is a strategy for goal-directed RL
with sparse reward. In CER, a pair of agents, \(\pi _A \) and \(\pi _B\),
are trained simultaneously.

Agent \(\pi _A\) is punished if agent \(\pi _B\) is also visited the
near states, \(|s_A^i - s_B^j| < \delta \). Whereas, agent \(\pi _B\)
gets reward if it visits near the agent \(\pi _A\). Such reward
re-labelling is executed per mini-batch.

Depending on initializing methods, there are two variants of
CER. Independent-CER initializes \(\pi _B\) with the task's initial
distribution. Interect-CER initializes \(\pi _B\) with random
off-policy sample of \(\pi _A\).


The authors pointed out that CER complemented the HER. Together with
HER, CER improves performance.

** With cpprb
Because of [[https://ymd_h.gitlab.io/cpprb/features/flexible_environment/][flexible environment design]], cpprb can store a pair of
transitions from 2 agents. However, the current version of cpprb
doesn't have the functionality of sampling episodes instead of
transitions.


** References
- [[https://iclr.cc/Conferences/2019/Schedule?showEvent=1091][H. Liu et. al., "Competitive Experience Replay", ICLR 2019]] ([[https://arxiv.org/abs/1902.00528][arXiv]])

* DONE Dynamic Experience Replay (DER)
CLOSED: [2021-02-01 Mon 08:25]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: survey
:EXPORT_FILE_NAME: der
:END:

** Overview
In [[http://proceedings.mlr.press/v100/luo20a.html][Dynamic Experience Replay (DER)]], not only human demonstrations but
also successful episodes can be used as demonstrations. For robot
control task, human demonstration is not always perfect demonstration,
so that DER auguments and replaces demonstrations by successful
episodes. DER can work with distributed RL framework such [[https://arxiv.org/abs/1803.00933][Ape-X]].


DER uses multiple (global) replay buffers
\(\lbrace\mathbb{B}_0\dots\mathbb{B}_n\rbrace\) which have
demonstration zone. On the episode end, explorers randomly picked one
of the replay buffers \(\mathbb{B}_i\) and stores
transisions. Additionally, if the episode succeeds, it is stored at
another (ordinary) replay buffer \(\mathbb{T}\) for successful
transitions.

Learner picks one of the replay buffers \(\mathbb{B}_j\) randomly,
then samples mini-batch, trains network, and updates priorities as
usual prioritized experience replay. Periodically, demonstration zones
are replaced by randomly sampled transitions from successful
transition replay buffer \(\mathbb{T}\).



** With cpprb
DER requires specialized demonstration zone where transisions are
keeped. Naively speaking, it can be realized by using 2 replay
buffers. However, to sample from this joint replay buffer with
prioritized way, cpprb need some modification. We might implement
something like =JointPrioritizedReplayBuffer=.


** References
- [[http://proceedings.mlr.press/v100/luo20a.html][J. Luo and H. Li, "Dynamic Experience Replay", CoRL (2020)]] ([[https://arxiv.org/abs/2003.02372][arXiv]], [[https://sites.google.com/site/dynamicexperiencereplay][site]])

* DONE Neural Experience Experience Replay Sampler (NERS)
CLOSED: [2021-02-13 Sat 11:03]
:PROPERTIES:
:EXPORT_FILE_NAME: ners
:EXPORT_HUGO_SECTION*: survey
:END:

** Overview
[[https://openreview.net/forum?id=gJYlaqL8i8][Neural Experience Replay Sampler (NERS)]] is neural network learns
sampling score \(\sigma _i\) for each transition. RL policy is trained
like PER, except using inferred score \(\sigma _i\) instead of
\(\mid\text{TD}\mid\).


\[ p_i = \frac{\sigma _i ^{\alpha}}{\sum _{k \in [\mid B\mid ]}}\sigma _k ^{\alpha} \]


NERS consists of 3 neural networks, local network \(f_l\), global
network \(f_g\), and score network \(f_s\). \(f_l\) calculates for
each transisiton. \(f_g\) calculates for each transition then these
outputs are averaged over minibatch as global context.\(f_s\) takes
the concatenation of the outputs of \(f_l\) and the global context and
calculates scores \(sigma _i\) for each transition.


NERS are trained at each episode end with subset transitions of that
used for updating of actor and critic during the episode.

In order to optimize replay reward
\(r ^{\text{re}} = \sum _{t \in \text{current episode}} r_t \sum _{t \in \text{previous episode}} r_t \),
the following gradients are used;

\[\nabla _{\phi} \mathbb{E} [r^{\text{re}}] = \mathbb{E}[r^{\text{re}}\sum _{i\in I_{\text{train}}} \nabla _{\phi} \log \sigma _i (D(I_{\text{i}}))] \]


** With cpprb
You can implement NERS with the current cpprb. For main replay buffer,
you can use =PrioritizedReplayBuffer=. For index collection, you can
use =ReplayBuffer=. Additionaly you need to implement neural network
to learn sampling probability.


** References
- [[https://openreview.net/forum?id=gJYlaqL8i8][Y. Oh et. al., "Learning to Sample with Local and Global Contexts in Experience Replay Buffer", ICRL (2021)]] ([[https://arxiv.org/abs/2007.07358][arXiv]])

* Footnotes

[fn:1] Updating segment tree for PER is critical section, too. To
avoid data race, ~MPPrioritizedReplayBuffer~ lazily updates segment
tree from learner process just before ~sample~ method.
